# Computer Vision on CIFAR-10: From Basics to Transformers

This repository provides an **end-to-end exploration of computer vision techniques** on the CIFAR-10 dataset.  

We begin with foundational methods (KNN, SVM, Softmax) and progress step by step to:  
- Neural networks implemented with barebone PyTorch and `nn.Module`  
- Experiments with activation functions, learning rates, optimizers, and hidden layer configurations  
- Custom CNNs compared against ResNet  
- Transfer learning and fine-tuning  
- Implementation of Vision Transformers (ViT) with 16×16 patch embeddings, inspired by the original paper  

---

## Contents
1. `1_knn.ipynb` – k-Nearest Neighbors on CIFAR-10  
2. `2_svm.ipynb` – Support Vector Machine  
3. `3_softmax.ipynb` – Softmax classifier  
4. `4_numpy_two_layer_nets.ipynb` – Two-layer neural net (NumPy implementation)  
5. `5_pytorch_two_layer_nets.ipynb` – Two-layer neural net (PyTorch implementation)  
6. `6_custom_cnns_vs_resnet.ipynb` – Custom CNNs vs ResNet  
7. `7_custom_vision_transformer_vs_ViT.ipynb` – Vision Transformers (ViT) from scratch  

---

## Purpose
A hands-on journey for both beginners and advanced practitioners to understand the **evolution of computer vision models**, from classic ML approaches to modern deep learning architectures.  
